@inproceedings{pan-etal-2021-zero,
 abstract = {Neural models for automated fact verification have achieved promising results thanks to the availability of large, human-annotated datasets. However, for each new domain that requires fact verification, creating a dataset by manually writing claims and linking them to their supporting evidence is expensive. We develop QACG, a framework for training a robust fact verification model by using automatically generated claims that can be supported, refuted, or unverifiable from evidence from Wikipedia. QACG generates question-answer pairs from the evidence and then converts them into different types of claims. Experiments on the FEVER dataset show that our QACG framework significantly reduces the demand for human-annotated training data. In a zero-shot scenario, QACG improves a RoBERTa modelâ€²s F1 from 50% to 77%, equivalent in performance to 2K+ manually-curated examples. Our QACG code is publicly available.},
 address = {Online},
 author = {Pan, Liangming  and
Chen, Wenhu  and
Xiong, Wenhan  and
Kan, Min-Yen  and
Wang, William Yang},
 booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 doi = {10.18653/v1/2021.acl-short.61},
 editor = {Zong, Chengqing  and
Xia, Fei  and
Li, Wenjie  and
Navigli, Roberto},
 month = {August},
 pages = {476--483},
 publisher = {Association for Computational Linguistics},
 title = {Zero-shot Fact Verification by Claim Generation},
 url = {https://aclanthology.org/2021.acl-short.61},
 year = {2021}
}
