@inproceedings{diao-etal-2023-doolittle,
 abstract = {Improving the quality of academic writing is a meaningful but challenging task. Conventional methods of language refinement focus on narrow, specific linguistic features within isolated sentences, such as grammatical errors and improper word use. We propose a more general task, Academic Writing Formalization (AWF), to improve the overall quality of formal academic writing at the paragraph level. We formulate this language refinement task as a formal text style transfer task which transfers informal-academic text to formal-academic and contribute a large-scale non-parallel dataset, Doolittle, for this purpose. Concurrently, we apply a method named metric-oriented reinforcement learning (MORL) to two large language models (LLM) where we incorporate different levels of automatic feedback into the training process. Our experiments reveal that existing text transfer models and grammatical error correction models address certain aspects of AWF but still have a significant performance gap compared to human performance. Meanwhile, language models fine-tuned with our MORL method exhibit considerably improved performance, rivaling the latest chatbot ChatGPT, but still have a non-negligible gap compared to the ground truth formal-academic texts in Doolittle.},
 address = {Singapore},
 author = {Diao, Shizhe  and
Lei, Yongyu  and
Pan, Liangming  and
Fang, Tianqing  and
Zhou, Wangchunshu  and
Keh, Sedrick  and
Kan, Min-Yen  and
Zhang, Tong},
 booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2023.emnlp-main.809},
 editor = {Bouamor, Houda  and
Pino, Juan  and
Bali, Kalika},
 month = {December},
 pages = {13093--13111},
 publisher = {Association for Computational Linguistics},
 title = {Doolittle: Benchmarks and Corpora for Academic Writing Formalization},
 url = {https://aclanthology.org/2023.emnlp-main.809},
 year = {2023}
}
